{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21: Image Processing with Neural Networks\n",
    "\n",
    "In this session, we will delve into the implementation of Multilayer Perceptrons (MLPs), a class of feedforward artificial neural networks, using the CIFAR-10 dataset. We'll explore the architecture of MLPs, understand the importance of activation functions, and see how they perform with image data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images from CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. [Learn more about CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to load the CIFAR-10 dataset from the data folder\n",
    "def load_cifar_batch(filename):\n",
    "    \"\"\"Load a single batch of CIFAR-10.\"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        # The encoding 'bytes' is required for Python 3 compatibility\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "        images = batch[b'data']\n",
    "        labels = batch[b'labels']\n",
    "        # Reshape the images: the dataset is flattened, so you need to reshape it to 32x32x3\n",
    "        images = images.reshape((len(images), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "        labels = np.array(labels)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batch 1 of CIFAR-10\n",
    "file_name = 'data/batch_1'\n",
    "\n",
    "images, labels = load_cifar_batch(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect images and labels\n",
    "\n",
    "Understanding the structure and format of our dataset is crucial. Let’s start by examining the lengths of images and labels to get a sense of the dataset's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at length of images and labels\n",
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the first three images\n",
    "\n",
    "Visualizing our data is just as important as understanding its structure. Let’s display the first few images from our dataset along with their corresponding labels to see what we’re working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show an image\n",
    "def show_image(img, label):\n",
    "    plt.imshow(img)\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "# Show the first three images\n",
    "for index in range(3):\n",
    "    print(index)\n",
    "    show_image(images[index], labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define labels\n",
    "Each image in CIFAR-10 is associated with a label from 10 classes. Here, we define a list of label names to make our data more understandable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the first label\n",
    "\n",
    "To further familiarize ourselves with the dataset, let's inspect the label of the first image. This step helps us connect an image with its categorical representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first label\n",
    "first_label = labels[0]\n",
    "\n",
    "label_names[first_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first image data\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "Data preprocessing is a critical step in any machine learning workflow. Here, we'll normalize pixel values to improve our model's convergence during training. We'll also reshape the data to fit our model's input requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "images_normalized = images / 255.0\n",
    "\n",
    "images_normalized[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the shape of the images\n",
    "images_normalized.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "images_flattened = images_normalized.reshape(images_normalized.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect flattened images\n",
    "images_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32 * 32 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_flattened, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it worked\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Machine Learning Model\n",
    "This time use an MLPClassifier (Neural Network) with 2 hidden layers with 32 nodes each. Use the 'relu' activation function, and a maximum of 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "After training the model, evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "\n",
    "# Calculate accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dataframe with loss value for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss_df with loss values for each iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 150 zero values to the loss_df so that other interation lengths (i.e. 200) can be added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with different hyperparameters\n",
    "First, expand the number of iterations to 200, and add loss values to the `loss_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the loss_df with the new loss values and plot the updated loss curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Hyperparameter Tuning a Neural Network\n",
    "\n",
    "In this activity, we will perform hyperparameter tuning for our [Multilayer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) model. Using `GridSearchCV`, we will search for the best combination of hyperparameters, including activation function, hidden layer sizes, learning rate, and the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models with different hidden layer sizes\n",
    "Using 200 iteration, train NN's with hidden layers of (64, 64) and (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with new (64 by 64) hidden layer sizes and 200 iterations (max_iter=200)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "# create loss_3 column in loss_df and plot the updated loss curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with new (128 by 128) hidden layer sizes and 200 iterations (max_iter=200), train it, and add to loss_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the updated loss curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a grid search with the following hyperparameters:\n",
    "hidden_layer_sizes: (128, 128), (64, 64), (32, 32) \n",
    "\n",
    "activation: tanh or relu\n",
    "\n",
    "learning_rate: constant or adaptive\n",
    "\n",
    "max_iter: 100\n",
    "\n",
    "Use three fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define params for the GridSearchCV\n",
    "\n",
    "\n",
    "# Initialize the model, and run grid search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the best hyperparamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with the best hyperparameters, add loss data to dataframe, and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the accuracy of the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
